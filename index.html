<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Editable Free-Viewpoint Video using a Layered Neural Representation</title>
  <link href="./css/style.css" rel="stylesheet" type="text/css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-159450849-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-159450849-2');
  </script>
  <meta name="description" content="Project page for 'Editable Free-Viewpoint Video using a Layered Neural Representation'">
</head>
<body>
  <p class="title">Editable Free-Viewpoint Video using a Layered Neural Representation</p>
  <p class="author">
    <span class="author"><a target="_blank" href="https://orcid.org/0000-0001-9477-3159">Jiakai Zhang</a>&nbsp;<sup>1,&nbsp;3</sup></span>
    <span class="author">Xinhang Liu</a>&nbsp;<sup>1</sup></span>
    <span class="author">Xinyi Ye</a>&nbsp;<sup>1</sup></span>
    <span class="author">Fuqiang Zhao</a>&nbsp;<sup>1</sup></span>
    <span class="author">Yanshun Zhang</a>&nbsp;<sup>2</sup></span>
    <span class="author"><a target="_blank" href="https://wuminye.com/">Minye Wu</a>&nbsp;<sup>1</sup></span>
    <span class="author"><a target="_blank" href="https://cn.linkedin.com/in/yingliangzhang">Yingliang Zhang</a>&nbsp;<sup>2</sup></span>
    <span class="author"><a target="_blank" href="http://xu-lan.com/">Lan Xu</a>&nbsp;<sup>1</sup></span>
    <span class="author"><a target="_blank" href="http://www.yu-jingyi.com/cv/">Jingyi Yu</a>&nbsp;<sup>1</sup></span>
  </p>
  <table border="0" align="center" class="affiliations" width="1200px">
    <tbody align="center">
      <tr>
        <td style="text-align: right"><img src="./assets/shanghaitech-red.png" height="48" alt=""></td>
        <td style="text-align: left">&nbsp;<sup>1</sup>&nbsp;<a href="https://www.shanghaitech.edu.cn">ShanghaiTech University</a></td>
        <td style="text-align: right"><img src="./assets/dgene.png" height="48" alt=""></td>
        <td style="text-align: left">&nbsp;<sup>2</sup>&nbsp;<a href="https://www.dgene.com/">DGene</a></td>
        <td style="text-align: right"><img src="./assets/stereye.png" height="48" alt=""></td>
        <td style="text-align: left">&nbsp;<sup>2</sup>&nbsp;<a href="https://www.stereye.com/">Stereye</a></td>
      </tr>
    </tbody></table>
    <table width="999" border="0" align="center" class="menu">
      <tbody>
        <tr>
          <td align="center">| <a href="#paper">Paper</a> | <a href="#video">Video</a> | <a href="#code">Code</a> |</td>
        </tr>
      </tbody>
    </table>

    <div class="container">
      <table width="200" border="0" align="center">
        <tbody>
          <tr>
            <td colspan="3" align="center"><img src="./assets/teaser1.gif" width="720" alt="" style="margin-top:10px"></td>
          </tr>
          <tr>
            <td class="label">Original Video<br>(Jump separately)</td>
            <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
            <td class="label">Our Retimed Result<br><span class="highlight">(Jump together!)</span></td>
          </tr>
        </tbody></table>
        <table width="200" border="0" align="center" style="table-layout: fixed; width: 100%">
          <tbody>
            <tr>
              <td colspan="4" class="caption"><p><b>Making all children jump into the pool together &mdash; in post-processing!</b> In the original video (left) each child is jumping into the pool at a different time. In our computationally retimed video (right), the jumps are aligned such that all the children jump together into the pool (notice that the child on the left remains unchanged in the input and output videos). In this paper, we present a method to produce this and other people retiming effects in natural, ordinary videos.
              </p></td>
            </tr>
            <tr>
              <td colspan="4"><img src="./assets/teaser2.gif" width="980" alt=""></td>
            </tr>
            <tr style="text-align:center">
              <td>Background Layer</td>
              <td>Layer 1</td>
              <td>Layer 2</td>
              <td>Layer 3</td>
            </tr>
            <tr>
              <td colspan="4" class="caption"><p><b>Decomposing a video into layers.</b> Our method is based on a novel deep neural network that learns a layered decomposition of the input video. Our model not only disentangles the motions of people in different layers, but can also capture the various scene elements that are <b>correlated</b> with those people (e.g., water splashes as the children hit the water, shadows, reflections). When people are retimed, those related elements are automatically retimed with them, which allows us to create realistic and faithful re-renderings of the video for a variety of retiming effects.
              </p></td>
            </tr>

          </tbody></table>
          <p><span class="section">Abstract</span> </p>
          <p>
          Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.
          </p>
          <table width="940" border="0" align="center" id="video">
            <tbody>
              <tr>
                <iframe width="940" src="//player.bilibili.com/player.html?aid=630291392&bvid=BV1e84y1F7YT&cid=332220112&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                </td>
              </tr>
            </tbody>
          </table>
          <p class="section" id="paper">Paper</p>
          <table width="940" border="0">
            <tbody>
              <tr>
                <td height="100"><a href="https://arxiv.org/pdf/2009.07833.pdf"><img src="./assets/paper_preview.png" alt="" width="140" height="167"></a></td>
                <td width="750"><p><b>Layered Neural Rendering for Retiming People in Video</b><br>
                  Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T. Freeman, and Michael Rubinstein<br>
                  <em>SIGGRAPH Asia 2020.</em><br><br>
                  [<a href="https://arxiv.org/pdf/2009.07833.pdf">paper</a>]</p>
                </td>
              </tr>
            </tbody>
          </table>
          <p class="section">&nbsp;</p>
          <p class="section">Supplementary Material</p>
          <table width="587" height="136" border="0">
            <tbody>
              <tr>
                <td width="245"><img src="./assets/supp_fig.png" alt="" height="150"></td>
                <td align="left">
                  <p>[<a href="./supplementary/index.html">supplementary page</a>]</p>
                </td>
              </tr>
            </tbody>
          </table>
          <p class="section">&nbsp;</p>
          <p class="section" id="code">Code</p>
          <table border="0">
            <tbody>
              <tr>
                <td width="80"><a href="https://github.com/google/retiming"><img src="./assets/github_logo.png" alt="" height="50"></a></td>
                <td align="left">
                  <p>[<a href="https://github.com/google/retiming">code</a>]</p>
                </td>
              </tr>
            </tbody>
          </table>
          <p>&nbsp;</p>
          <blockquote>
            <p>&nbsp;</p>
            <p>&nbsp;</p>
            <p><em><b>Acknowledgements. </b>The original "Ballroom" video belongs to Desert Classic Dance. This work was funded in part by the EPSRC Programme Grant Seebibyte EP/M013774/1.
            </em>  </p>
          </blockquote>
        </div>


      </body></html>
